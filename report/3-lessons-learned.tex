\section{Lessons learned perspective} \label{section:Lessons learned perspective}
\todo{Link back to respective commit messages, issues, tickets, etc. to illustrate these.}
\subsection{Issues: evolution and refactoring}
Initially, the inherited code was translated to a basic Java application using the Java Spark framework\footnote{\url{https://github.com/DevOps2021-gb/devops2021/issues/152}}. At the time, this code was very bare-bones with all logic and endpoints located in a single file. As time went on, the team refactored the application to make use of a more well structured system architecture (controller-, service-, persistance-layer)\footnote{\url{https://github.com/DevOps2021-gb/devops2021/tree/ae6cd48a06fde2a5c403056b33895b7589cb039d}} eventually adding dependency injection and mock testing\footnote{\url{https://github.com/DevOps2021-gb/devops2021/commit/07914442a09e204f0be2687fe8e5b9bd07799ee4}} to the application. 
During development one of the endpoints response code was changed which led to a massive increase in errors the simulator reported, this could have been avoided if a fraction of the provided \texttt{minitwit\_simulator.py} file had been a part of the test pipeline.

\subsection{Issues: operation}
\label{issues-operation}
The self-made heartbeat protocol had some issues related to Spark. An issue\footnote{\url{https://github.com/DevOps2021-gb/devops2021/issues/171}} was created trying to remedy this but was never finished, rendering the availability part of the system unstable as the backup would perceive the primary to be down then reassign the floating-ip to itself exposing a single point of failure. The issue is likely related to Spark's session handling.


Early on in the project our MySQL database was deployed together with our minitwit application using Vagrant. Due to SSH key mismanagement, we lost access to our vagrant deployed DigitalOcean minitwit server containing simulator generated data. Luckily we were able to reset the root user through digital ocean's web terminal functionality. Through this terminal we could add SSH public keys to the \textit{authorized\_keys} file. Finally, we could then SSH into the server and get a database dump. This database dump was based on an outdated model with no ORM, so we then had to alter the tables before transferring it to the new remote database server. Only a minor amount of data was lost during the data transfer. To ensure minor data loss in the future we created a backup Github Action that made a database backup once a day. In addition, SSH keys for the server were backed up in a keystore.
On the 18th of March our webserver crashed due to SSL exceptions. After looking through Docker logs, we narrowed it down to an incompatibility between java-mysql-connector 8.0.15 and Java 11. The solution to this was to update java-mysql-connector 8.0.15 to 8.0.23 \footnote{\url{https://github.com/DevOps2021-gb/devops2021/issues/94}}.

\subsection{Issues: maintenance}
The group never figured out how make SonarCloud read the test coverage\footnote{\url{https://github.com/DevOps2021-gb/devops2021/issues/140}}, eventually scrapping the idea after spending much time trying to make it work.

Furthermore, the group tried to implement alert notifications on Grafana\footnote{\url{https://github.com/DevOps2021-gb/devops2021/issues/87}} in order to be notified of unexpected data from the monitoring but this was never successfully implemented.